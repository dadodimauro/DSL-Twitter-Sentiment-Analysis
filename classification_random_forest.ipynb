{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06983d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4af71137",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(\"./clean_df.csv\", usecols=[\"ids\", \"user\", \"text\", \"sentiment\", \"month\", \"dayofweek\", \"time\", \"n_tweet\", \"positive_emo\", \"negative_emo\", \"positive_word\", \"negative_word\"])\n",
    "df = pd.read_csv(\"./clean_df.csv\", usecols=[\"ids\", \"user\", \"text\", \"sentiment\", \"dayofweek\", \"hour\", \"min\", \"n_tweet\", \"positive_emo\", \"negative_emo\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b21bb714",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>ids</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>hour</th>\n",
       "      <th>min</th>\n",
       "      <th>n_tweet</th>\n",
       "      <th>positive_emo</th>\n",
       "      <th>negative_emo</th>\n",
       "      <th>positive_word</th>\n",
       "      <th>negative_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1833972543</td>\n",
       "      <td>Killandra</td>\n",
       "      <td>ye talk help a lot go through it there no judg...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1980318193</td>\n",
       "      <td>IMlisacowan</td>\n",
       "      <td>sunshin live it ima lie on the gra listen to o...</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1994409198</td>\n",
       "      <td>yaseminx3</td>\n",
       "      <td>someth for your iphon</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>52</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1824749377</td>\n",
       "      <td>no_surprises</td>\n",
       "      <td>couldnt get in to the after parti</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2001199113</td>\n",
       "      <td>Rhi_ShortStack</td>\n",
       "      <td>aw is andi be mean again now i want maca</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment         ids            user  \\\n",
       "0        1.0  1833972543       Killandra   \n",
       "1        1.0  1980318193     IMlisacowan   \n",
       "2        1.0  1994409198       yaseminx3   \n",
       "3        0.0  1824749377    no_surprises   \n",
       "4        0.0  2001199113  Rhi_ShortStack   \n",
       "\n",
       "                                                text  dayofweek  hour  min  \\\n",
       "0  ye talk help a lot go through it there no judg...          0     1    8   \n",
       "1  sunshin live it ima lie on the gra listen to o...          6     6   23   \n",
       "2                              someth for your iphon          0    11   52   \n",
       "3                  couldnt get in to the after parti          6     2   45   \n",
       "4           aw is andi be mean again now i want maca          1     0    8   \n",
       "\n",
       "   n_tweet  positive_emo  negative_emo  positive_word  negative_word  \n",
       "0       28             0             0              0              0  \n",
       "1       28             0             0              1              1  \n",
       "2       96             0             0              0              0  \n",
       "3       24             0             0              0              0  \n",
       "4       47             0             0              0              0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aca5475a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 299993 entries, 0 to 299992\n",
      "Data columns (total 12 columns):\n",
      " #   Column         Non-Null Count   Dtype  \n",
      "---  ------         --------------   -----  \n",
      " 0   sentiment      224994 non-null  float64\n",
      " 1   ids            299993 non-null  int64  \n",
      " 2   user           299993 non-null  object \n",
      " 3   text           299993 non-null  object \n",
      " 4   dayofweek      299993 non-null  int64  \n",
      " 5   hour           299993 non-null  int64  \n",
      " 6   min            299993 non-null  int64  \n",
      " 7   n_tweet        299993 non-null  int64  \n",
      " 8   positive_emo   299993 non-null  int64  \n",
      " 9   negative_emo   299993 non-null  int64  \n",
      " 10  positive_word  299993 non-null  int64  \n",
      " 11  negative_word  299993 non-null  int64  \n",
      "dtypes: float64(1), int64(9), object(2)\n",
      "memory usage: 27.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bda8a3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df = df.dropna(subset=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fb70499",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 299993 entries, 0 to 299992\n",
      "Data columns (total 12 columns):\n",
      " #   Column         Non-Null Count   Dtype  \n",
      "---  ------         --------------   -----  \n",
      " 0   sentiment      224994 non-null  float64\n",
      " 1   ids            299993 non-null  int64  \n",
      " 2   user           299993 non-null  object \n",
      " 3   text           299993 non-null  object \n",
      " 4   dayofweek      299993 non-null  int64  \n",
      " 5   hour           299993 non-null  int64  \n",
      " 6   min            299993 non-null  int64  \n",
      " 7   n_tweet        299993 non-null  int64  \n",
      " 8   positive_emo   299993 non-null  int64  \n",
      " 9   negative_emo   299993 non-null  int64  \n",
      " 10  positive_word  299993 non-null  int64  \n",
      " 11  negative_word  299993 non-null  int64  \n",
      "dtypes: float64(1), int64(9), object(2)\n",
      "memory usage: 27.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7c90ee03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "max_features = 300\n",
    "#tfidconv = TfidfVectorizer(use_idf=True, stop_words=\"english\", max_features=max_features, min_df=5, max_df=0.7)\n",
    "#tfidconv = TfidfVectorizer(use_idf=True, stop_words=\"english\", max_features=max_features)\n",
    "tfidconv = TfidfVectorizer(use_idf=True, max_features=max_features)\n",
    "tfIdf = tfidconv.fit_transform(df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9c6e6f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfIdf = pd.DataFrame(tfIdf.toarray(), index=df.index.values, columns=[f\"word_{w}\" for w in tfidconv.get_feature_names_out()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "abf2eb83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 299993 entries, 0 to 299992\n",
      "Columns: 300 entries, word_about to word_your\n",
      "dtypes: float64(300)\n",
      "memory usage: 688.9 MB\n"
     ]
    }
   ],
   "source": [
    "df_tfIdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "48f951f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>hour</th>\n",
       "      <th>min</th>\n",
       "      <th>n_tweet</th>\n",
       "      <th>positive_emo</th>\n",
       "      <th>negative_emo</th>\n",
       "      <th>positive_word</th>\n",
       "      <th>negative_word</th>\n",
       "      <th>word_about</th>\n",
       "      <th>...</th>\n",
       "      <th>word_would</th>\n",
       "      <th>word_wow</th>\n",
       "      <th>word_ya</th>\n",
       "      <th>word_yay</th>\n",
       "      <th>word_ye</th>\n",
       "      <th>word_yeah</th>\n",
       "      <th>word_year</th>\n",
       "      <th>word_yet</th>\n",
       "      <th>word_you</th>\n",
       "      <th>word_your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.29259</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.154369</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>52</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.506892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 309 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment  dayofweek  hour  min  n_tweet  positive_emo  negative_emo  \\\n",
       "0        1.0          0     1    8       28             0             0   \n",
       "1        1.0          6     6   23       28             0             0   \n",
       "2        1.0          0    11   52       96             0             0   \n",
       "3        0.0          6     2   45       24             0             0   \n",
       "4        0.0          1     0    8       47             0             0   \n",
       "\n",
       "   positive_word  negative_word  word_about  ...  word_would  word_wow  \\\n",
       "0              0              0         0.0  ...         0.0       0.0   \n",
       "1              1              1         0.0  ...         0.0       0.0   \n",
       "2              0              0         0.0  ...         0.0       0.0   \n",
       "3              0              0         0.0  ...         0.0       0.0   \n",
       "4              0              0         0.0  ...         0.0       0.0   \n",
       "\n",
       "   word_ya  word_yay  word_ye  word_yeah  word_year  word_yet  word_you  \\\n",
       "0      0.0       0.0  0.29259        0.0        0.0       0.0  0.154369   \n",
       "1      0.0       0.0  0.00000        0.0        0.0       0.0  0.000000   \n",
       "2      0.0       0.0  0.00000        0.0        0.0       0.0  0.000000   \n",
       "3      0.0       0.0  0.00000        0.0        0.0       0.0  0.000000   \n",
       "4      0.0       0.0  0.00000        0.0        0.0       0.0  0.000000   \n",
       "\n",
       "   word_your  \n",
       "0   0.000000  \n",
       "1   0.000000  \n",
       "2   0.506892  \n",
       "3   0.000000  \n",
       "4   0.000000  \n",
       "\n",
       "[5 rows x 309 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final = pd.concat([df, df_tfIdf], axis=1)\n",
    "df_final = df_final.drop([\"text\", \"user\", \"ids\"], axis=1)\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eff10ad8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 299993 entries, 0 to 299992\n",
      "Columns: 309 entries, sentiment to word_your\n",
      "dtypes: float64(301), int64(8)\n",
      "memory usage: 707.2 MB\n"
     ]
    }
   ],
   "source": [
    "df_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f525666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_mask = ~df_final[\"sentiment\"].isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7911088b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_final.drop(columns=[\"sentiment\"]).values\n",
    "y = df_final[\"sentiment\"].values\n",
    "\n",
    "X_train_valid = X[train_valid_mask]\n",
    "y_train_valid = y[train_valid_mask]\n",
    "X_test = X[~train_valid_mask]\n",
    "y_test = y[~train_valid_mask]\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3d2c772b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train_valid)\n",
    "X_train_valid = scaler.transform(X_train_valid)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e83f6bbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.69      0.71     23791\n",
      "         1.0       0.78      0.82      0.80     32458\n",
      "\n",
      "    accuracy                           0.77     56249\n",
      "   macro avg       0.76      0.75      0.76     56249\n",
      "weighted avg       0.76      0.77      0.76     56249\n",
      "\n",
      "0.7570446775506654\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "clf = RandomForestClassifier(100, random_state=42, criterion=\"entropy\", max_features=\"log2\")\n",
    "clf.fit(X_train, np.array(y_train))\n",
    "y_pred = clf.predict(X_valid)\n",
    "\n",
    "print(classification_report(y_valid, y_pred))\n",
    "print(f1_score(y_valid, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9bd8f9db",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('n_tweet', 0.041123744152675536),\n",
       " ('min', 0.038326584114002284),\n",
       " ('negative_word', 0.03666022145253874),\n",
       " ('hour', 0.034789364184323573),\n",
       " ('dayofweek', 0.024430414312403722),\n",
       " ('word_you', 0.016014609998223975),\n",
       " ('word_to', 0.015237457744151882),\n",
       " ('word_the', 0.014362903012260973),\n",
       " ('word_thank', 0.01273654061565117),\n",
       " ('word_it', 0.012395704216894479),\n",
       " ('positive_word', 0.012375301215084707),\n",
       " ('word_not', 0.01162439004190635),\n",
       " ('word_my', 0.011382541256670868),\n",
       " ('word_and', 0.010117504492124079),\n",
       " ('word_that', 0.009391293515345954),\n",
       " ('word_no', 0.00921770982983724),\n",
       " ('word_is', 0.009194051802913075),\n",
       " ('word_me', 0.008953779832728576),\n",
       " ('word_of', 0.00885822585961018),\n",
       " ('word_for', 0.008845106974101185),\n",
       " ('word_in', 0.008413578358180981),\n",
       " ('word_but', 0.008380404528757029),\n",
       " ('word_im', 0.008336436538382445),\n",
       " ('word_god', 0.008038913244428729),\n",
       " ('word_have', 0.007887023605419466),\n",
       " ('word_on', 0.007548256844584209),\n",
       " ('word_dont', 0.0074979043849879145),\n",
       " ('word_love', 0.0074635963328494636),\n",
       " ('word_sad', 0.007192937952983707),\n",
       " ('word_cant', 0.007122998860820196),\n",
       " ('word_so', 0.007009975632277626),\n",
       " ('word_your', 0.006642642572587359),\n",
       " ('word_lol', 0.006613928343893378),\n",
       " ('word_just', 0.006559305289460632),\n",
       " ('word_be', 0.0063399361789235435),\n",
       " ('word_go', 0.005981654065347912),\n",
       " ('word_sori', 0.005808758088043344),\n",
       " ('word_with', 0.0055113198437130115),\n",
       " ('word_wa', 0.005465658790221393),\n",
       " ('word_wish', 0.005459257973452306),\n",
       " ('word_work', 0.0053813623960193326),\n",
       " ('word_now', 0.005327946436533216),\n",
       " ('word_want', 0.005160964632133665),\n",
       " ('word_up', 0.005093511534291619),\n",
       " ('word_at', 0.004906797514163674),\n",
       " ('word_do', 0.004837409530587397),\n",
       " ('word_mi', 0.0047965819567054755),\n",
       " ('word_get', 0.004791107075549518),\n",
       " ('word_are', 0.00472272767586526),\n",
       " ('word_what', 0.004535845326241325),\n",
       " ('word_al', 0.0045060940652651964),\n",
       " ('word_like', 0.004485407611888303),\n",
       " ('word_thi', 0.0044759748795493715),\n",
       " ('word_know', 0.004402703317118708),\n",
       " ('word_oh', 0.004333743499857249),\n",
       " ('word_out', 0.004197344614420898),\n",
       " ('word_didnt', 0.00410964530063951),\n",
       " ('word_hate', 0.004037612134977714),\n",
       " ('word_day', 0.004030155249198385),\n",
       " ('word_fel', 0.003954000984448318),\n",
       " ('word_haha', 0.00392465962909327),\n",
       " ('word_whi', 0.0038366025539758614),\n",
       " ('word_suck', 0.0038307989546480653),\n",
       " ('word_aw', 0.003723446685233641),\n",
       " ('word_think', 0.003705694369311331),\n",
       " ('word_one', 0.003653245051673618),\n",
       " ('word_got', 0.003565487827326568),\n",
       " ('word_ned', 0.00356403335271911),\n",
       " ('word_great', 0.0035550015703169095),\n",
       " ('word_time', 0.003543742405125235),\n",
       " ('word_wil', 0.0035415490340828426),\n",
       " ('word_am', 0.003459682022823169),\n",
       " ('word_how', 0.0033714103840935005),\n",
       " ('word_wel', 0.0033259551067415195),\n",
       " ('word_about', 0.0033188469267987773),\n",
       " ('negative_emo', 0.0033102804956417426),\n",
       " ('word_from', 0.003292510550310195),\n",
       " ('word_can', 0.0032901114318429985),\n",
       " ('word_stil', 0.0032871365041350314),\n",
       " ('word_if', 0.0032329632759709754),\n",
       " ('word_we', 0.003227277259021719),\n",
       " ('word_today', 0.0032243359380769608),\n",
       " ('word_there', 0.003214466503947986),\n",
       " ('word_morn', 0.0032084932185458665),\n",
       " ('word_reali', 0.00320316886206233),\n",
       " ('word_back', 0.00317230810885246),\n",
       " ('word_se', 0.0031310366167123095),\n",
       " ('word_wait', 0.00307096351464852),\n",
       " ('word_bad', 0.0030602716813838858),\n",
       " ('word_some', 0.002993971453194205),\n",
       " ('word_folow', 0.0029697271192691706),\n",
       " ('word_had', 0.002932110555173117),\n",
       " ('word_watch', 0.0028838511945010726),\n",
       " ('word_they', 0.0028677677170692957),\n",
       " ('word_here', 0.0028470863956806215),\n",
       " ('word_yeah', 0.0027871712017960336),\n",
       " ('word_he', 0.0027576517555831072),\n",
       " ('word_ugh', 0.0027563126023410433),\n",
       " ('word_ha', 0.002700777057883709),\n",
       " ('word_hope', 0.0026840282725495057),\n",
       " ('word_as', 0.0026826262500442167),\n",
       " ('word_hurt', 0.0026789259808905003),\n",
       " ('word_make', 0.002663026364502479),\n",
       " ('word_hey', 0.0026604189067820005),\n",
       " ('word_when', 0.0026477146044509947),\n",
       " ('word_awesom', 0.0026392500623467134),\n",
       " ('word_wont', 0.0026181922120200756),\n",
       " ('word_mise', 0.0025981706876150273),\n",
       " ('word_come', 0.0025571241242575136),\n",
       " ('word_or', 0.002514976603176757),\n",
       " ('word_ben', 0.0025091271923689436),\n",
       " ('word_onli', 0.0024947240713612765),\n",
       " ('word_il', 0.0024606954177415594),\n",
       " ('word_ye', 0.002421820431518408),\n",
       " ('word_night', 0.0024134771861955197),\n",
       " ('word_more', 0.002413241237883503),\n",
       " ('word_then', 0.0023939083465061113),\n",
       " ('word_much', 0.0023894993897464484),\n",
       " ('word_yay', 0.002315686672837372),\n",
       " ('word_though', 0.0022959773327055895),\n",
       " ('word_nice', 0.0022657012339825926),\n",
       " ('word_new', 0.002264735201622584),\n",
       " ('word_hapi', 0.002227931037064055),\n",
       " ('word_by', 0.002206534877190885),\n",
       " ('word_twiter', 0.0021725855574042064),\n",
       " ('word_an', 0.0021571561914872483),\n",
       " ('word_say', 0.00215529118704135),\n",
       " ('word_right', 0.0021504718675520672),\n",
       " ('word_did', 0.0021404186427667333),\n",
       " ('word_wana', 0.002131300815771979),\n",
       " ('word_would', 0.002110924697747857),\n",
       " ('word_hi', 0.0020963910827202954),\n",
       " ('word_she', 0.0020874098164389665),\n",
       " ('word_tri', 0.0020755645716285616),\n",
       " ('word_again', 0.0020618580631118788),\n",
       " ('word_never', 0.002032902408317579),\n",
       " ('word_doesnt', 0.0020322937443099484),\n",
       " ('word_tomorow', 0.0019985595928992887),\n",
       " ('word_home', 0.001984977779859467),\n",
       " ('word_sick', 0.0019692762326941685),\n",
       " ('word_them', 0.0019644968002264446),\n",
       " ('word_fun', 0.0019603921142138385),\n",
       " ('word_last', 0.0019340360835557444),\n",
       " ('word_ive', 0.0019271339851833192),\n",
       " ('word_col', 0.001883607682981725),\n",
       " ('word_cri', 0.0018733998983867712),\n",
       " ('word_pleas', 0.0018657000286939971),\n",
       " ('word_thing', 0.0018639081687394177),\n",
       " ('word_ok', 0.0018473020237813503),\n",
       " ('word_were', 0.0018269444428947654),\n",
       " ('word_lost', 0.0018164224648507237),\n",
       " ('word_where', 0.0017881237825619907),\n",
       " ('word_way', 0.0017616360514415507),\n",
       " ('word_take', 0.0017579422955746543),\n",
       " ('word_her', 0.0017567802680648575),\n",
       " ('word_lok', 0.0017565314596752718),\n",
       " ('word_down', 0.0017513798505974765),\n",
       " ('word_over', 0.001749739566157028),\n",
       " ('word_twet', 0.0017441800990300018),\n",
       " ('word_damn', 0.0017429468385596364),\n",
       " ('word_veri', 0.001740063476856714),\n",
       " ('word_find', 0.0017308645786304705),\n",
       " ('word_slep', 0.0017249745972307464),\n",
       " ('word_rain', 0.0017210381380531148),\n",
       " ('word_could', 0.0017123697759561635),\n",
       " ('word_even', 0.001690578542552049),\n",
       " ('word_should', 0.0016863716070376073),\n",
       " ('word_beter', 0.0016560888020685119),\n",
       " ('word_sure', 0.00163808839790832),\n",
       " ('word_havent', 0.0016319960278185813),\n",
       " ('word_gona', 0.0015732484211011387),\n",
       " ('word_welcom', 0.001571097812996715),\n",
       " ('word_who', 0.0015661603156119025),\n",
       " ('word_hour', 0.001532055767114156),\n",
       " ('word_friend', 0.0014912856876984487),\n",
       " ('word_son', 0.0014887853437624695),\n",
       " ('word_bore', 0.0014672399375910727),\n",
       " ('word_use', 0.0014665048319943237),\n",
       " ('word_peopl', 0.001456942888272852),\n",
       " ('word_gete', 0.0014558270854153938),\n",
       " ('word_isnt', 0.0014517067782914184),\n",
       " ('positive_emo', 0.0014374063734213685),\n",
       " ('word_schol', 0.0014334187908907983),\n",
       " ('word_start', 0.001425342907193128),\n",
       " ('word_everyon', 0.0013942183251494163),\n",
       " ('word_omg', 0.00138448877472499),\n",
       " ('word_ah', 0.0013843571850278432),\n",
       " ('word_him', 0.0013643205757801588),\n",
       " ('word_wek', 0.0013524640960379775),\n",
       " ('word_tire', 0.0013466974973813843),\n",
       " ('word_bed', 0.0013379177365766429),\n",
       " ('word_long', 0.0013367788173962768),\n",
       " ('word_alway', 0.0013231132508324831),\n",
       " ('word_help', 0.0013220843577798397),\n",
       " ('word_mean', 0.0013180658798275363),\n",
       " ('word_enjoy', 0.0013137842792346071),\n",
       " ('word_guy', 0.0013077487474726722),\n",
       " ('word_let', 0.0013046795007212722),\n",
       " ('word_fele', 0.0013008760122199504),\n",
       " ('word_tonight', 0.0012944985249567385),\n",
       " ('word_away', 0.0012702803312017235),\n",
       " ('word_ur', 0.0012617875707621113),\n",
       " ('word_talk', 0.0012475964464265842),\n",
       " ('word_sound', 0.001239604406895825),\n",
       " ('word_thought', 0.0012367537548339056),\n",
       " ('word_girl', 0.0012240249615969193),\n",
       " ('word_next', 0.0012223036015345913),\n",
       " ('word_someth', 0.0012189252713188533),\n",
       " ('word_yet', 0.0012137819315478817),\n",
       " ('word_ani', 0.0012120674238062542),\n",
       " ('word_ya', 0.0012086256857100801),\n",
       " ('word_bit', 0.0012025926980713574),\n",
       " ('word_hapen', 0.0012010295825429246),\n",
       " ('word_litl', 0.0011964856211617331),\n",
       " ('word_alreadi', 0.0011962131387020722),\n",
       " ('word_eat', 0.0011901914441932362),\n",
       " ('word_same', 0.0011871449329586706),\n",
       " ('word_play', 0.001181715681373364),\n",
       " ('word_song', 0.001176308214147687),\n",
       " ('word_show', 0.0011755653795835405),\n",
       " ('word_glad', 0.0011630312714404293),\n",
       " ('word_fuck', 0.00114553320546087),\n",
       " ('word_man', 0.0011358502767252053),\n",
       " ('word_later', 0.0011324175739647962),\n",
       " ('word_check', 0.0011319251509633668),\n",
       " ('word_listen', 0.0011312819280067652),\n",
       " ('word_done', 0.001130328084813643),\n",
       " ('word_read', 0.0011235686985934482),\n",
       " ('word_leav', 0.0011176261989635943),\n",
       " ('word_after', 0.0011021799756013443),\n",
       " ('word_live', 0.0010911581249635014),\n",
       " ('word_tel', 0.0010869655342870114),\n",
       " ('word_mayb', 0.0010844737242247098),\n",
       " ('word_noth', 0.0010836791909331108),\n",
       " ('word_us', 0.0010824991412175628),\n",
       " ('word_than', 0.001081662709677946),\n",
       " ('word_okay', 0.0010806584757291362),\n",
       " ('word_hahaha', 0.001075216249938755),\n",
       " ('word_lot', 0.0010691955831779426),\n",
       " ('word_said', 0.001067880501428606),\n",
       " ('word_kep', 0.0010435433172761405),\n",
       " ('word_loke', 0.0010347984015635753),\n",
       " ('word_hug', 0.0010218667904191667),\n",
       " ('word_year', 0.0010213900514479844),\n",
       " ('word_gue', 0.0010210196285273818),\n",
       " ('word_other', 0.0010206844392785155),\n",
       " ('word_first', 0.0010169250256173506),\n",
       " ('word_hot', 0.0010159556179694163),\n",
       " ('word_went', 0.0010134526588433993),\n",
       " ('word_phone', 0.0010122924812113105),\n",
       " ('word_best', 0.0010062939507287719),\n",
       " ('word_tho', 0.0010057321731415751),\n",
       " ('word_cute', 0.0009976653575870097),\n",
       " ('word_movi', 0.00099437147969244),\n",
       " ('word_our', 0.0009892505164909957),\n",
       " ('word_becaus', 0.0009890341114008275),\n",
       " ('word_head', 0.000986792423443366),\n",
       " ('word_someon', 0.0009607359940215486),\n",
       " ('word_ad', 0.0009586372186993615),\n",
       " ('word_pic', 0.0009495481775378483),\n",
       " ('word_made', 0.0009492591531765117),\n",
       " ('word_sinc', 0.0009404218126016274),\n",
       " ('word_actuali', 0.0009326467965010459),\n",
       " ('word_hear', 0.0009282558309018523),\n",
       " ('word_anoth', 0.0009257464932696005),\n",
       " ('word_befor', 0.0009191243870063516),\n",
       " ('word_late', 0.0009155697010664015),\n",
       " ('word_swet', 0.0009128973154355986),\n",
       " ('word_give', 0.0009099827507547131),\n",
       " ('word_life', 0.0008972880613938102),\n",
       " ('word_doe', 0.0008969575138913298),\n",
       " ('word_excit', 0.0008964426372857484),\n",
       " ('word_those', 0.0008693084563921052),\n",
       " ('word_id', 0.0008403880876201359),\n",
       " ('word_big', 0.0008350731948934082),\n",
       " ('word_hard', 0.0008326063952335087),\n",
       " ('word_might', 0.0008323174116422081),\n",
       " ('word_into', 0.0008321727906621934),\n",
       " ('word_preti', 0.0008294666026076305),\n",
       " ('word_til', 0.0008225004788418985),\n",
       " ('word_wow', 0.0008108623089141909),\n",
       " ('word_amaz', 0.0007922603025778184),\n",
       " ('word_end', 0.0007787816047958743),\n",
       " ('word_gota', 0.0007737406397272088),\n",
       " ('word_finish', 0.0007719710567329023),\n",
       " ('word_wekend', 0.0007710785087060856),\n",
       " ('word_onc', 0.0007684380994783982),\n",
       " ('word_old', 0.0007484500848311137),\n",
       " ('word_ever', 0.0007446042900454755),\n",
       " ('word_stuf', 0.0007439581807006261),\n",
       " ('word_mani', 0.0007390420348546807),\n",
       " ('word_few', 0.0007380556519949082),\n",
       " ('word_also', 0.0007359352025519187),\n",
       " ('word_while', 0.0007358916933527245),\n",
       " ('word_anyth', 0.0007326198355250866),\n",
       " ('word_post', 0.0007253334715341442),\n",
       " ('word_must', 0.0007220954062335687),\n",
       " ('word_hous', 0.0007183158703809806),\n",
       " ('word_th', 0.0007143002659585286),\n",
       " ('word_send', 0.0006981101557690225),\n",
       " ('word_saw', 0.0006885750003443271),\n",
       " ('word_cal', 0.0006709617587939729),\n",
       " ('word_video', 0.0006488853243364384),\n",
       " ('word_music', 0.0006422795971986003),\n",
       " ('word_pay', 0.0006168462663216076),\n",
       " ('word_wonder', 0.0006132482551565571),\n",
       " ('word_put', 0.0006038901926508688),\n",
       " ('word_train', 0.0005690945992804827)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = [ feauture for feauture in df_final.columns[1:] ]\n",
    "sorted(zip(feature_names, clf.feature_importances_), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "62ef1a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from sklearn.model_selection import train_test_split\\n\\nX = df_final.drop(columns=[\"sentiment\", \"month\", \"dayofweek\", \"time\", \"n_tweet\", \"positive_emo\", \"negative_emo\", \"positive_word\", \"negative_word\"]).values\\ny = df_final[\"sentiment\"].values\\n\\nX_train_valid = X[train_valid_mask]\\ny_train_valid = y[train_valid_mask]\\nX_test = X[~train_valid_mask]\\ny_test = y[~train_valid_mask]\\n\\nX_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, shuffle=True, random_state=42)'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_final.drop(columns=[\"sentiment\", \"month\", \"dayofweek\", \"time\", \"n_tweet\", \"positive_emo\", \"negative_emo\", \"positive_word\", \"negative_word\"]).values\n",
    "y = df_final[\"sentiment\"].values\n",
    "\n",
    "X_train_valid = X[train_valid_mask]\n",
    "y_train_valid = y[train_valid_mask]\n",
    "X_test = X[~train_valid_mask]\n",
    "y_test = y[~train_valid_mask]\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, shuffle=True, random_state=42)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f05b7fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(100, random_state=42, criterion=\"entropy\", max_features=\"log2\")\n",
    "clf.fit(X_train_valid, np.array(y_train_valid))\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "pd.DataFrame(y_pred).to_csv(\"output.csv\", index_label=\"Id\", header=[\"Predicted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "34ad241a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from sklearn.model_selection import GridSearchCV\\n\\nparam_grid = {\\n    \"n_estimators\": [10, 50, 100],\\n    \"criterion\": [\"gini\", \"entropy\"],\\n    \"max_features\": [\"auto\", \"sqrt\", \"log2\"],\\n    \"random_state\": [42],# always use the samet random seed\\n    \"n_jobs\": [-1],# for parallelization\\n}\\ngs = GridSearchCV(RandomForestClassifier(), param_grid, scoring=\"f1_macro\", n_jobs=-1, cv=5)\\n\\ngs.fit(X_train_valid, y_train_valid)\\ngs.best_score_'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": [10, 50, 100],\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "    \"random_state\": [42],# always use the samet random seed\n",
    "    \"n_jobs\": [-1],# for parallelization\n",
    "}\n",
    "gs = GridSearchCV(RandomForestClassifier(), param_grid, scoring=\"f1_macro\", n_jobs=-1, cv=5)\n",
    "\n",
    "gs.fit(X_train_valid, y_train_valid)\n",
    "gs.best_score_\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a98776d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'y_pred = gs.predict(X_test)\\n\\npd.DataFrame(y_pred, index=df[~train_valid_mask].index).to_csv(\"output.csv\", index_label=\"Id\", header=[\"Predicted\"])'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"y_pred = gs.predict(X_test)\n",
    "\n",
    "pd.DataFrame(y_pred, index=df[~train_valid_mask].index).to_csv(\"output_rf.csv\", index_label=\"Id\", header=[\"Predicted\"])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f6277646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.68      0.65      0.67     23791\n",
      "         1.0       0.75      0.78      0.76     32458\n",
      "\n",
      "    accuracy                           0.72     56249\n",
      "   macro avg       0.72      0.71      0.72     56249\n",
      "weighted avg       0.72      0.72      0.72     56249\n",
      "\n",
      "0.7159294078654215\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(max_iter = 500, random_state=42)\n",
    "mlp.fit(X_train, np.array(y_train))\n",
    "y_pred = mlp.predict(X_valid)\n",
    "\n",
    "print(classification_report(y_valid, y_pred))\n",
    "print(f1_score(y_valid, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6a72ffc8",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.76      0.67      0.71     23791\n",
      "         1.0       0.78      0.84      0.81     32458\n",
      "\n",
      "    accuracy                           0.77     56249\n",
      "   macro avg       0.77      0.76      0.76     56249\n",
      "weighted avg       0.77      0.77      0.77     56249\n",
      "\n",
      "0.7613155667132387\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC(random_state=42)\n",
    "svc.fit(X_train, np.array(y_train))\n",
    "y_pred = svc.predict(X_valid)\n",
    "\n",
    "print(classification_report(y_valid, y_pred))\n",
    "print(f1_score(y_valid, y_pred, average='macro'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}